{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, csv, math, random\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import decord\n",
    "from decord import VideoReader\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/video-generation/AnimateDiff')\n",
    "# import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from animatediff.utils.util import zero_rank_print\n",
    "\n",
    "\n",
    "\n",
    "class Something_v2(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            csv_path, video_folder,\n",
    "            sample_size=256, sample_stride=4, sample_n_frames=16,\n",
    "            is_image=False,\n",
    "        ):\n",
    "        zero_rank_print(f\"loading annotations from {csv_path} ...\")\n",
    "        \n",
    "        self.dataset = []\n",
    "        with open(csv_path, 'r') as f:\n",
    "            annotations = json.load(f)\n",
    "\n",
    "            for item in annotations:\n",
    "                video_id = item['id']\n",
    "                template = item['template']\n",
    "                placeholders = item['placeholders']\n",
    "                \n",
    "                prompt = template\n",
    "                for placeholder in placeholders:\n",
    "                    prompt = prompt.replace('[something]', placeholder, 1)\n",
    "                    \n",
    "                self.dataset.append({\"videoid\": video_id, \"name\": prompt, \"template\": template, \"placeholders\": placeholders})\n",
    "            \n",
    "            \n",
    "            \n",
    "        self.length = len(self.dataset)\n",
    "        zero_rank_print(f\"data scale: {self.length}\")\n",
    "\n",
    "        self.video_folder    = video_folder\n",
    "        self.sample_stride   = sample_stride\n",
    "        self.sample_n_frames = sample_n_frames\n",
    "        self.is_image        = is_image\n",
    "        \n",
    "        sample_size = tuple(sample_size) if not isinstance(sample_size, int) else (sample_size, sample_size)\n",
    "        self.pixel_transforms = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Resize(sample_size[0]),\n",
    "            transforms.CenterCrop(sample_size),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True),\n",
    "        ])\n",
    "    \n",
    "    def get_batch(self, idx):\n",
    "        video_dict = self.dataset[idx]\n",
    "        videoid, name, _, _ = video_dict['videoid'], video_dict['name'], video_dict['template'], video_dict[\"placeholders\"]\n",
    "        \n",
    "        video_dir    = os.path.join(self.video_folder, f\"{videoid}.webm\")\n",
    "        video_reader = VideoReader(video_dir, num_threads=1)\n",
    "        video_length = len(video_reader)\n",
    "        \n",
    "        # # Khởi tạo VideoReader\n",
    "        # try:\n",
    "        #     vr = VideoReader(video_dir)\n",
    "        #     print(f\"Video có {len(vr)} khung hình\")\n",
    "        #     # Lấy khung hình đầu tiên\n",
    "        #     frame = vr[0]\n",
    "        #     print(\"Frame đầu tiên:\", frame.shape)\n",
    "        # except decord.DECORDError as e:\n",
    "        #     print(f\"Không thể đọc video: {e}\")\n",
    "        \n",
    "        if not self.is_image:\n",
    "            clip_length = min(video_length, (self.sample_n_frames - 1) * self.sample_stride + 1)\n",
    "            start_idx   = random.randint(0, video_length - clip_length)\n",
    "            batch_index = np.linspace(start_idx, start_idx + clip_length - 1, self.sample_n_frames, dtype=int)\n",
    "            print(batch_index)\n",
    "            print(self.sample_n_frames)\n",
    "        else:\n",
    "            batch_index = [random.randint(0, video_length - 1)]\n",
    "\n",
    "        # print(video_reader[1:4])\n",
    "        pixel_values = torch.from_numpy(video_reader.get_batch(batch_index).numpy()).permute(0, 3, 1, 2).contiguous()\n",
    "        pixel_values = pixel_values / 255.\n",
    "        del video_reader\n",
    "\n",
    "        if self.is_image:\n",
    "            pixel_values = pixel_values[0]\n",
    "        \n",
    "        return pixel_values, name\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # while True:\n",
    "        #     try:\n",
    "        pixel_values, name = self.get_batch(idx)\n",
    "                # break\n",
    "\n",
    "            # except Exception as e:\n",
    "            #     idx = random.randint(0, self.length-1)\n",
    "\n",
    "        pixel_values = self.pixel_transforms(pixel_values)\n",
    "        sample = dict(pixel_values=pixel_values, text=name)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56]\n",
      "20\n",
      "torch.Size([20, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "dataset = Something_v2(\n",
    "        csv_path=\"/home/ubuntu/video-generation/something_something_v2/data/labels/validation.json\",\n",
    "        video_folder=\"/home/ubuntu/video-generation/something_something_v2/data/20bn-something-something-v2\",\n",
    "        sample_size=256,\n",
    "        sample_stride=1, sample_n_frames=20,\n",
    "        is_image=False,\n",
    "    )\n",
    "\n",
    "print(dataset[0]['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/animated/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/animated/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/anaconda3/envs/animated/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_31483/3085530526.py\", line 100, in __getitem__\n    pixel_values, name = self.get_batch(idx)\n  File \"/tmp/ipykernel_31483/3085530526.py\", line 85, in get_batch\n    pixel_values = torch.from_numpy(video_reader.get_batch(batch_index).asnumpy()).permute(0, 3, 1, 2).contiguous()\nAttributeError: 'Tensor' object has no attribute 'asnumpy'. Did you mean: 'numpy'?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# import pdb\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# pdb.set_trace()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# for i in range(batch[\"pixel_values\"].shape[0]):\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#     save_videos_grid(batch[\"pixel_values\"][i:i+1].permute(0,2,1,3,4), os.path.join(\".\", f\"{idx}-{i}.mp4\"), rescale=True)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/animated/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/animated/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/animated/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/animated/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/animated/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/animated/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/anaconda3/envs/animated/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_31483/3085530526.py\", line 100, in __getitem__\n    pixel_values, name = self.get_batch(idx)\n  File \"/tmp/ipykernel_31483/3085530526.py\", line 85, in get_batch\n    pixel_values = torch.from_numpy(video_reader.get_batch(batch_index).asnumpy()).permute(0, 3, 1, 2).contiguous()\nAttributeError: 'Tensor' object has no attribute 'asnumpy'. Did you mean: 'numpy'?\n"
     ]
    }
   ],
   "source": [
    "dataset = Something_v2(\n",
    "        csv_path=\"/home/ubuntu/video-generation/something_something_v2/data/labels/validation.json\",\n",
    "        video_folder=\"/home/ubuntu/video-generation/something_something_v2/data/20bn-something-something-v2\",\n",
    "        sample_size=256,\n",
    "        sample_stride=4, sample_n_frames=10,\n",
    "        is_image=True,\n",
    "    )\n",
    "# import pdb\n",
    "# pdb.set_trace()\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, num_workers=16,)\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    print(batch[\"pixel_values\"].shape, len(batch[\"text\"]))\n",
    "    # for i in range(batch[\"pixel_values\"].shape[0]):\n",
    "    #     save_videos_grid(batch[\"pixel_values\"][i:i+1].permute(0,2,1,3,4), os.path.join(\".\", f\"{idx}-{i}.mp4\"), rescale=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '198186', 'prompt': 'Wiping words off of a paper', 'template': 'Wiping [something] off of [something]', 'placeholders': ['words', 'a paper']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "annotation_file = '/home/ubuntu/video-generation/something_something_v2/data/labels/validation.json'\n",
    "video_dict = []\n",
    "\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "    for item in annotations:\n",
    "        video_id = item['id']\n",
    "        template = item['template']\n",
    "        placeholders = item['placeholders']\n",
    "        \n",
    "        prompt = template\n",
    "        for placeholder in placeholders:\n",
    "            prompt = prompt.replace('[something]', placeholder, 1)\n",
    "            \n",
    "        # print(prompt)\n",
    "        video_dict.append({\"id\": video_id, \"prompt\": prompt, \"template\": template, \"placeholders\": placeholders})\n",
    "\n",
    "print(video_dict[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 381/220847 [00:02<23:20, 157.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(video_folder, filename)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Mở video bằng OpenCV\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoCapture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Kiểm tra xem video có mở được không\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Đường dẫn đến thư mục chứa video\n",
    "video_folder = '/home/ubuntu/video-generation/something_something_v2/data/20bn-something-something-v2'\n",
    "\n",
    "# Biến để đếm tổng số khung hình và số video\n",
    "total_frames = 0\n",
    "total_videos = 0\n",
    "min_frames = 99999999999\n",
    "max_frames = 0\n",
    "max_fps = 99999999999\n",
    "min_fps = 0\n",
    "# Lặp qua tất cả các file trong thư mục\n",
    "for id, filename in enumerate(tqdm(os.listdir(video_folder))):\n",
    "    if id > 50000:\n",
    "        break\n",
    "    if filename.endswith(\".webm\"):\n",
    "        video_path = os.path.join(video_folder, filename)\n",
    "        \n",
    "        # Mở video bằng OpenCV\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        # Kiểm tra xem video có mở được không\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Không thể mở video: {filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Lấy số lượng khung hình trong video\n",
    "        frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        min_frames = min(min_frames, frames)\n",
    "        max_frames = max(max_frames, frames)\n",
    "        \n",
    "        min_fps = min(min_fps, cv2.CAP_PROP_FPS)\n",
    "        max_fps = max(max_fps, cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        total_frames += frames\n",
    "        total_videos += 1\n",
    "        \n",
    "        # Đóng video sau khi xử lý\n",
    "        cap.release()\n",
    "\n",
    "# Tính số khung hình trung bình trên mỗi video\n",
    "if total_videos > 0:\n",
    "    average_frames = total_frames / total_videos\n",
    "    print(f\"Trung bình số khung hình trên mỗi video: {average_frames}\")\n",
    "    print(f\"Ít frames nhất: {min_frames}\")\n",
    "    print(f\"Nhiều frames nhất: {max_frames}\")\n",
    "    print(f\"Min FPS: {min_frames}\")\n",
    "    print(f\"Max FPS: {max_frames}\")\n",
    "\n",
    "else:\n",
    "    print(\"Không có video nào được tìm thấy trong thư mục.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video có 47 khung hình\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Out of bound indices: [50]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Lấy khung hình đầu tiên\u001b[39;00m\n\u001b[1;32m     12\u001b[0m frame \u001b[38;5;241m=\u001b[39m vr[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mvr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# frame = vr.get_batch([1, 4])\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrame đầu tiên:\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/animated/lib/python3.10/site-packages/decord/video_reader.py:174\u001b[0m, in \u001b[0;36mVideoReader.get_batch\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get entire batch of images. `get_batch` is optimized to handle seeking internally.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03mDuplicate frame indices will be optmized by copying existing frames rather than decode\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03mfrom video again.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m indices \u001b[38;5;241m=\u001b[39m _nd\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    175\u001b[0m arr \u001b[38;5;241m=\u001b[39m _CAPI_VideoReaderGetBatch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle, indices)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bridge_out(arr)\n",
      "File \u001b[0;32m~/anaconda3/envs/animated/lib/python3.10/site-packages/decord/video_reader.py:132\u001b[0m, in \u001b[0;36mVideoReader._validate_indices\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid negative indices: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(indices[indices \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_frame))\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (indices \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_frame)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOut of bound indices: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(indices[indices \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_frame]))\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "\u001b[0;31mIndexError\u001b[0m: Out of bound indices: [50]"
     ]
    }
   ],
   "source": [
    "import decord\n",
    "from decord import VideoReader\n",
    "decord.bridge.set_bridge('torch')  # nếu bạn muốn làm việc với Tensor trong PyTorch\n",
    "\n",
    "video_path = '/home/ubuntu/video-generation/something_something_v2/data/20bn-something-something-v2/1.webm'\n",
    "\n",
    "# Khởi tạo VideoReader\n",
    "try:\n",
    "    vr = VideoReader(video_path)\n",
    "    print(f\"Video có {len(vr)} khung hình\")\n",
    "    # Lấy khung hình đầu tiên\n",
    "    frame = vr[0]\n",
    "    frame = vr.get_batch([50])\n",
    "    # frame = vr.get_batch([1, 4])\n",
    "    print(\"Frame đầu tiên:\", frame.shape)\n",
    "except decord.DECORDError as e:\n",
    "    print(f\"Không thể đọc video: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_templates = [\n",
    "#     'Approaching [something] with your camera',\n",
    "#     'Closing [something]',\n",
    "#     'Dropping [something]',\n",
    "#     'Folding [something]',\n",
    "#     'Holding [something]',\n",
    "#     'Holding [something] next to [something]',\n",
    "#     'Moving [something] away from [something]',\n",
    "#     'Moving [something] away from the camera',\n",
    "#     'Moving [something] closer to [something]',\n",
    "#     'Moving [something] down',\n",
    "#     'Moving [something] from left to right',\n",
    "#     'Moving [something] from right to left',\n",
    "#     'Moving [something] towards the camera',\n",
    "#     'Moving away from [something] with your camera',\n",
    "#     'Opening [something]',\n",
    "#     'Picking [something]',\n",
    "#     'Plugging [something] into [something]',\n",
    "#     'Poking [something]',\n",
    "#     'Pouring [something]',\n",
    "#     'Pushing [something] so that it slightly moves',\n",
    "# ]\n",
    "\n",
    "target_templates = [\n",
    "    'Approaching [something] with your camera',\n",
    "    'Closing [something]',\n",
    "    'Dropping [something]',\n",
    "    'Holding [something] next to [something]',\n",
    "    'Moving [something] away from [something]',\n",
    "    'Moving [something] towards the camera',\n",
    "    'Moving [something] from left to right',\n",
    "    'Opening [something]',\n",
    "    'Picking [something]',\n",
    "    'Pouring [something]'\n",
    "]\n",
    "\n",
    "import json\n",
    "with open('/home/ubuntu/video-generation/something_something_v2/data/labels/train.json', 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "    \n",
    "filtered_data = [entry for entry in train_data if entry[\"template\"] in target_templates]\n",
    "\n",
    "# Save the filtered data to a new JSON file\n",
    "with open('/home/ubuntu/video-generation/something_something_v2/data/labels/10_class_train.json', 'w') as outfile:\n",
    "    json.dump(filtered_data, outfile, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17754\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
